{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Setup + Document Loading\n",
    "\n",
    "**Goal**: Load DataFlow's enterprise documents for RAG system\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ Knowledge Base Path: /Users/krishnaashili/git/pragatidev/AIAgentsBootcamp-master/Section_6_Real_World_RAG_Engineering/enterprise_knowledge_base\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Imports and setup\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from langchain.document_loaders import TextLoader, CSVLoader, JSONLoader, UnstructuredMarkdownLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Path to enterprise documents\n",
    "#KNOWLEDGE_BASE_PATH = Path(\"Section_6_Real_World_RAG_Engineering/enterprise_knowledge_base\")\n",
    "KNOWLEDGE_BASE_PATH = Path(\"enterprise_knowledge_base\")\n",
    "print(f\"ğŸ“‚ Knowledge Base Path: {KNOWLEDGE_BASE_PATH.resolve()}\")\n",
    "print(\"âœ… Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading DataFlow's documents...\n",
      "ğŸ“ business_data...\n",
      "   âœ… integration_partners.csv\n",
      "   âœ… customer_analytics.csv\n",
      "   âœ… billing_and_pricing.csv\n",
      "ğŸ“ internal_operations...\n",
      "   âœ… support_operations/customer_support_procedures.markdown\n",
      "   âœ… support_operations/system_architecture.markdown\n",
      "   âœ… sales_marketing/sales_playbook.json\n",
      "   âœ… hr_policies/onboarding_checklist.json\n",
      "   âœ… hr_policies/employee_handbook.txt\n",
      "   âœ… product_releases/release_notes.json\n",
      "ğŸ“ customer_facing...\n",
      "   âœ… api_documentation.json\n",
      "   âœ… troubleshooting_guide.txt\n",
      "   âœ… terms_of_service.markdown\n",
      "   âœ… product_user_guide.markdown\n",
      "   âœ… competitive_analysis.txt\n",
      "ğŸ“ legal_compliance...\n",
      "   âœ… compliance_certifications.csv\n",
      "   âœ… security_policies.txt\n",
      "   âœ… terms_of_service.markdown\n",
      "   âœ… privacy_policy.txt\n",
      "\n",
      "ğŸ¯ LOADED: 212 documents from 4 departments\n",
      "ğŸ“Š Content: 262,596 characters\n",
      "ğŸ¢ Departments: business_data, customer_facing, internal_operations, legal_compliance\n"
     ]
    }
   ],
   "source": [
    "def load_enterprise_documents(base_path: Path) -> List[Document]:\n",
    "    \"\"\"Load all documents recursively with proper metadata\"\"\"\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    print(\"ğŸ”„ Loading DataFlow's documents...\")\n",
    "    \n",
    "    # Process each department folder\n",
    "    for dept_path in base_path.iterdir():\n",
    "        if not dept_path.is_dir():\n",
    "            continue\n",
    "            \n",
    "        department = dept_path.name\n",
    "        print(f\"ğŸ“ {department}...\")\n",
    "        \n",
    "        # Get ALL files recursively\n",
    "        files = [f for f in dept_path.rglob(\"*\") if f.is_file()]\n",
    "        \n",
    "        for file_path in files:\n",
    "            try:\n",
    "                # Choose loader by extension\n",
    "                ext = file_path.suffix.lower()\n",
    "                if ext == '.csv':\n",
    "                    loader = CSVLoader(str(file_path))\n",
    "                elif ext == '.json':\n",
    "                    loader = JSONLoader(str(file_path), jq_schema='.', text_content=False)\n",
    "                elif ext == '.md':\n",
    "                    loader = UnstructuredMarkdownLoader(str(file_path))\n",
    "                else:\n",
    "                    loader = TextLoader(str(file_path), encoding='utf-8')\n",
    "                \n",
    "                # Load and add metadata\n",
    "                docs = loader.load()\n",
    "                for doc in docs:\n",
    "                    doc.metadata.update({\n",
    "                        \"department\": department,\n",
    "                        \"source_file\": file_path.name,\n",
    "                        \"file_type\": ext\n",
    "                    })\n",
    "                \n",
    "                all_docs.extend(docs)\n",
    "                rel_path = file_path.relative_to(dept_path)\n",
    "                print(f\"   âœ… {rel_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ {file_path.name}: {str(e)[:30]}...\")\n",
    "    \n",
    "    # Quick summary\n",
    "    departments = set(doc.metadata['department'] for doc in all_docs)\n",
    "    total_chars = sum(len(doc.page_content) for doc in all_docs)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ LOADED: {len(all_docs)} documents from {len(departments)} departments\")\n",
    "    print(f\"ğŸ“Š Content: {total_chars:,} characters\")\n",
    "    print(f\"ğŸ¢ Departments: {', '.join(sorted(departments))}\")\n",
    "    \n",
    "    return all_docs\n",
    "\n",
    "# Load all documents\n",
    "documents = load_enterprise_documents(KNOWLEDGE_BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Validation:\n",
      "   Documents: 212  \n",
      "   Departments: 4  \n",
      "   Content: 262,596  \n",
      "\n",
      "âœ… SUCCESS! Ready for Part 2: Text Chunking\n"
     ]
    }
   ],
   "source": [
    "# Quick validation\n",
    "print(\"ğŸ” Validation:\")\n",
    "print(f\"   Documents: {len(documents)}  \")\n",
    "print(f\"   Departments: {len(set(doc.metadata['department'] for doc in documents))}  \")\n",
    "print(f\"   Content: {sum(len(doc.page_content) for doc in documents):,}  \")\n",
    "\n",
    "if len(documents) >= 15:\n",
    "    print(\"\\nâœ… SUCCESS! Ready for Part 2: Text Chunking\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Low document count - check folder structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Complete!\n",
    "\n",
    "**âœ… You've loaded DataFlow's complete knowledge base**\n",
    "\n",
    "**Ready for Part 2:** Text chunking (the critical RAG skill)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Text Chunking\n",
    "\n",
    "**Goal**: Transform 212 documents into optimally-sized chunks for RAG\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Text chunking tools imported!\n",
      "ğŸ“„ Starting with 212 documents from Part 1\n"
     ]
    }
   ],
   "source": [
    "# Imports for text chunking\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "print(\"âœ… Text chunking tools imported!\")\n",
    "print(f\"ğŸ“„ Starting with {len(documents)} documents from Part 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smart Chunking Strategy\n",
    "\n",
    "**Industry Best Practice**: 1000 characters with 200 overlap\n",
    "- **Why 1000 chars?** Perfect balance for embedding models\n",
    "- **Why 200 overlap?** Preserves context across chunks\n",
    "- **Recursive splitting**: Tries sentences, then words, then characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Creating smart chunks...\n",
      "ğŸ“ legal_compliance: 28 docs â†’ 80 chunks\n",
      "ğŸ“ business_data: 173 docs â†’ 173 chunks\n",
      "ğŸ“ customer_facing: 5 docs â†’ 105 chunks\n",
      "ğŸ“ internal_operations: 6 docs â†’ 119 chunks\n",
      "\n",
      "ğŸ¯ CHUNKING COMPLETE:\n",
      "   ğŸ“„ Original: 212 documents\n",
      "   âœ‚ï¸ Created: 477 chunks\n",
      "   ğŸ“Š Ratio: 2.2 chunks per document\n"
     ]
    }
   ],
   "source": [
    "def create_smart_chunks(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"Split documents into optimal chunks for RAG\"\"\"\n",
    "    \n",
    "    print(\"âœ‚ï¸ Creating smart chunks...\")\n",
    "    \n",
    "    # Industry-standard chunking settings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,        # Optimal for embedding models\n",
    "        chunk_overlap=200,      # Preserve context\n",
    "        length_function=len,    # Character-based\n",
    "        separators=[            # Try these in order:\n",
    "            \"\\n\\n\",              # Paragraphs first\n",
    "            \"\\n\",                # Then lines\n",
    "            \". \",                # Then sentences\n",
    "            \" \",                 # Then words\n",
    "            \"\",                  # Finally characters\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    stats = {\n",
    "        \"original_docs\": len(documents),\n",
    "        \"total_chunks\": 0,\n",
    "        \"by_department\": {},\n",
    "        \"by_file_type\": {}\n",
    "    }\n",
    "    \n",
    "    # Process each department\n",
    "    for dept in set(doc.metadata['department'] for doc in documents):\n",
    "        dept_docs = [doc for doc in documents if doc.metadata['department'] == dept]\n",
    "        dept_chunks = []\n",
    "        \n",
    "        print(f\"ğŸ“ {dept}: {len(dept_docs)} docs â†’ \", end=\"\")\n",
    "        \n",
    "        for doc in dept_docs:\n",
    "            # Split the document\n",
    "            chunks = text_splitter.split_documents([doc])\n",
    "            \n",
    "            # Add chunk metadata\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunk.metadata.update({\n",
    "                    \"chunk_id\": f\"{doc.metadata['source_file']}_{i}\",\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"chunk_size\": len(chunk.page_content)\n",
    "                })\n",
    "            \n",
    "            dept_chunks.extend(chunks)\n",
    "            \n",
    "            # Track stats\n",
    "            file_type = doc.metadata.get('file_type', 'unknown')\n",
    "            stats[\"by_file_type\"][file_type] = stats[\"by_file_type\"].get(file_type, 0) + len(chunks)\n",
    "        \n",
    "        stats[\"by_department\"][dept] = len(dept_chunks)\n",
    "        stats[\"total_chunks\"] += len(dept_chunks)\n",
    "        all_chunks.extend(dept_chunks)\n",
    "        \n",
    "        print(f\"{len(dept_chunks)} chunks\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ CHUNKING COMPLETE:\")\n",
    "    print(f\"   ğŸ“„ Original: {stats['original_docs']} documents\")\n",
    "    print(f\"   âœ‚ï¸ Created: {stats['total_chunks']} chunks\")\n",
    "    print(f\"   ğŸ“Š Ratio: {stats['total_chunks'] / stats['original_docs']:.1f} chunks per document\")\n",
    "    \n",
    "    return all_chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = create_smart_chunks(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Complete! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Vector Embeddings & Search\n",
    "\n",
    "**Goal**: Transform text chunks into searchable mathematical vectors\n",
    "**Why Critical**: This enables semantic search - finding meaning, not just keywords\n",
    " \n",
    "## What You'll Learn\n",
    "- Convert text to numerical vectors (embeddings)\n",
    "- Build production vector database with FAISS\n",
    "- Implement semantic search\n",
    "- Test retrieval accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using modern langchain-huggingface (recommended)\n",
      "âœ… Vector tools imported!\n",
      "ğŸ“Š Ready to embed 477 chunks from Part 2\n"
     ]
    }
   ],
   "source": [
    "# Modern imports (no deprecation warnings)\n",
    "try:\n",
    "    # Modern approach - no deprecation warnings\n",
    "    from langchain_huggingface import HuggingFaceEmbeddings\n",
    "    print(\"âœ… Using modern langchain-huggingface (recommended)\")\n",
    "    modern_import = True\n",
    "except ImportError:\n",
    "    # Fallback to deprecated version if needed\n",
    "    from langchain.embeddings import HuggingFaceEmbeddings\n",
    "    print(\"âš ï¸ Using deprecated import (consider upgrading)\")\n",
    "    print(\"ğŸ’¡ Run: pip install langchain-huggingface\")\n",
    "    modern_import = False\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "print(\"âœ… Vector tools imported!\")\n",
    "print(f\"ğŸ“Š Ready to embed {len(chunks)} chunks from Part 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Embedding Model\n",
    "\n",
    "**Model Choice**: `all-MiniLM-L6-v2`\n",
    "- **Fast**: Perfect for development and production\n",
    "- **Accurate**: Great semantic understanding\n",
    "- **Compact**: 384 dimensions (vs 1536 for OpenAI)\n",
    "- **Free**: No API costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§  Loading embedding model...\n",
      "âœ… Model loaded: sentence-transformers/all-MiniLM-L6-v2\n",
      "ğŸ“ Vector dimensions: 384\n",
      "âš¡ Device: CPU (production compatible)\n",
      "ğŸ‰ Using modern non-deprecated embeddings!\n"
     ]
    }
   ],
   "source": [
    "def setup_embedding_model():\n",
    "    \"\"\"Initialize the embedding model for vector creation\"\"\"\n",
    "    \n",
    "    print(\"ğŸ§  Loading embedding model...\")\n",
    "    \n",
    "    # Use production-grade embedding model\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # Modern LangChain wrapper (no deprecation warnings)\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={'device': 'cpu'},  # Use CPU for compatibility\n",
    "        encode_kwargs={'normalize_embeddings': True}  # Better for similarity search\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Model loaded: {model_name}\")\n",
    "    print(f\"ğŸ“ Vector dimensions: 384\")\n",
    "    print(f\"âš¡ Device: CPU (production compatible)\")\n",
    "    \n",
    "    if modern_import:\n",
    "        print(\"ğŸ‰ Using modern non-deprecated embeddings!\")\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Setup embeddings\n",
    "embeddings = setup_embedding_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector Store\n",
    "\n",
    "**FAISS**: Facebook's vector search library\n",
    "- Powers Instagram recommendations\n",
    "- Billion-scale vector search\n",
    "- Lightning-fast similarity search\n",
    "- Industry standard for RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Creating vector embeddings...\n",
      "â³ This may take 30-60 seconds...\n",
      "âœ… Vector store created!\n",
      "ğŸ“Š Vectors: 477\n",
      "ğŸ“ Dimensions: 384 per vector\n",
      "ğŸ’¾ Total size: ~0.7 MB\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(chunks: List, embeddings) -> FAISS:\n",
    "    \"\"\"Create FAISS vector store from text chunks\"\"\"\n",
    "    \n",
    "    print(\"ğŸ”¢ Creating vector embeddings...\")\n",
    "    print(\"â³ This may take 30-60 seconds...\")\n",
    "    \n",
    "    # Create vector store with FAISS\n",
    "    vector_store = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… Vector store created!\")\n",
    "    print(f\"ğŸ“Š Vectors: {len(chunks)}\")\n",
    "    print(f\"ğŸ“ Dimensions: 384 per vector\")\n",
    "    print(f\"ğŸ’¾ Total size: ~{len(chunks) * 384 * 4 / 1024 / 1024:.1f} MB\")\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = create_vector_store(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving vector store to 'dataflow_vector_store'...\n",
      "âœ… Vector store saved successfully!\n",
      "ğŸ“ Location: dataflow_vector_store/\n",
      "ğŸ”„ Can be loaded later with: FAISS.load_local('dataflow_vector_store', embeddings)\n"
     ]
    }
   ],
   "source": [
    "# Save vector store for production use\n",
    "def save_vector_store(vector_store: FAISS, save_path: str = \"dataflow_vector_store\"):\n",
    "    \"\"\"Save vector store to disk for reuse\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ’¾ Saving vector store to '{save_path}'...\")\n",
    "    \n",
    "    try:\n",
    "        vector_store.save_local(save_path)\n",
    "        print(f\"âœ… Vector store saved successfully!\")\n",
    "        print(f\"ğŸ“ Location: {save_path}/\")\n",
    "        print(f\"ğŸ”„ Can be loaded later with: FAISS.load_local('{save_path}', embeddings)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Save failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Save the vector store\n",
    "save_success = save_vector_store(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: RAG Agent - Complete Intelligent System\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Ollama LLM connected successfully!\n",
      "ğŸ†“ Using free local LLM\n",
      "Hello from Ollama:  How can I assist you today?\n",
      "ğŸ“Š Vector store ready: 477 chunks\n"
     ]
    }
   ],
   "source": [
    "# Imports for RAG agent\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import time\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Simple Ollama setup\n",
    "llm = None\n",
    "\n",
    "try:\n",
    "    from langchain.llms import Ollama\n",
    "    llm = Ollama(model=\"llama3.2\", base_url=\"http://localhost:11434\")\n",
    "    # Test connection\n",
    "    test_response = llm.invoke(\"Hello\")\n",
    "    print(\"âœ… Ollama LLM connected successfully!\")\n",
    "    print(\"ğŸ†“ Using free local LLM\")\n",
    "    print(\"Hello from Ollama: \", test_response)\n",
    "    print(f\"ğŸ“Š Vector store ready: {len(chunks)} chunks\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama connection failed: {e}\")\n",
    "    print(\"ğŸ’¡ Make sure Ollama is running: ollama serve\")\n",
    "    print(\"ğŸ’¡ And model is downloaded: ollama pull llama3.2\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Customer Service Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Professional customer service prompt created\n",
      "ğŸ¯ Optimized for helpful, accurate responses\n"
     ]
    }
   ],
   "source": [
    "# Professional customer service prompt\n",
    "CUSTOMER_SERVICE_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are DataFlow's helpful customer service assistant. Your job is to provide accurate, friendly, and professional support to customers.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use the provided context to answer questions accurately\n",
    "- Be concise but thorough in your explanations\n",
    "- If information isn't in the context, say \"I don't have that specific information\" and suggest contacting support\n",
    "- Always maintain a helpful and professional tone\n",
    "- For technical questions, provide step-by-step guidance when possible\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "CUSTOMER QUESTION:\n",
    "{question}\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Professional customer service prompt created\")\n",
    "print(\"ğŸ¯ Optimized for helpful, accurate responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— Creating RAG chain...\n",
      "âœ… RAG chain created successfully!\n",
      "ğŸ” Retriever: Top 4 most relevant chunks\n",
      "ğŸ¤– LLM: Ready for customer questions\n",
      "ğŸ“š Source attribution: Enabled\n"
     ]
    }
   ],
   "source": [
    "def create_rag_chain(vector_store, llm, prompt_template):\n",
    "    \"\"\"Create production RAG chain\"\"\"\n",
    "    \n",
    "    if not llm:\n",
    "        print(\"âŒ No LLM available - cannot create RAG chain\")\n",
    "        print(\"ğŸ’¡ Please install Ollama or set up OpenAI API key\")\n",
    "        return None\n",
    "    \n",
    "    print(\"ğŸ”— Creating RAG chain...\")\n",
    "    \n",
    "    # Create retrieval QA chain\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",  # Stuff all context into prompt\n",
    "        retriever=vector_store.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
    "        ),\n",
    "        chain_type_kwargs={\n",
    "            \"prompt\": prompt_template\n",
    "        },\n",
    "        return_source_documents=True  # Show which documents were used\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… RAG chain created successfully!\")\n",
    "    print(\"ğŸ” Retriever: Top 4 most relevant chunks\")\n",
    "    print(\"ğŸ¤– LLM: Ready for customer questions\")\n",
    "    print(\"ğŸ“š Source attribution: Enabled\")\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain = create_rag_chain(vector_store, llm, CUSTOMER_SERVICE_PROMPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Service Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– DataFlow Customer Service Agent initialized\n",
      "âœ… Customer service agent ready!\n"
     ]
    }
   ],
   "source": [
    "class DataFlowCustomerAgent:\n",
    "    \"\"\"Professional customer service agent with simple conversation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_chain):\n",
    "        self.rag_chain = rag_chain\n",
    "        # Simple conversation tracking (no deprecated memory)\n",
    "        self.conversation_history = []\n",
    "        self.conversation_count = 0\n",
    "        self.response_times = []\n",
    "        \n",
    "        print(\"ğŸ¤– DataFlow Customer Service Agent initialized\")\n",
    "    \n",
    "    def ask(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Ask the agent a question and get a comprehensive response\"\"\"\n",
    "        \n",
    "        if not self.rag_chain:\n",
    "            return {\n",
    "                \"answer\": \"I'm sorry, but I'm not properly configured right now. Please contact our support team directly.\",\n",
    "                \"sources\": [],\n",
    "                \"response_time\": 0,\n",
    "                \"error\": \"No LLM available\"\n",
    "            }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Get response from RAG chain\n",
    "            response = self.rag_chain.invoke({\"query\": question})\n",
    "            \n",
    "            end_time = time.time()\n",
    "            response_time = end_time - start_time\n",
    "            \n",
    "            # Simple conversation tracking\n",
    "            self.conversation_history.append({\n",
    "                \"question\": question,\n",
    "                \"answer\": response[\"result\"],\n",
    "                \"timestamp\": start_time\n",
    "            })\n",
    "            \n",
    "            # Track metrics\n",
    "            self.conversation_count += 1\n",
    "            self.response_times.append(response_time)\n",
    "            \n",
    "            # Extract source information\n",
    "            sources = []\n",
    "            if \"source_documents\" in response:\n",
    "                for doc in response[\"source_documents\"]:\n",
    "                    sources.append({\n",
    "                        \"department\": doc.metadata.get(\"department\", \"unknown\"),\n",
    "                        \"file\": doc.metadata.get(\"source_file\", \"unknown\"),\n",
    "                        \"preview\": doc.page_content[:100] + \"...\"\n",
    "                    })\n",
    "            \n",
    "            return {\n",
    "                \"answer\": response[\"result\"],\n",
    "                \"sources\": sources,\n",
    "                \"response_time\": response_time,\n",
    "                \"conversation_turn\": self.conversation_count\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"answer\": f\"I apologize, but I encountered an error. Please try rephrasing or contact support.\",\n",
    "                \"sources\": [],\n",
    "                \"response_time\": time.time() - start_time,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get agent performance statistics\"\"\"\n",
    "        \n",
    "        if not self.response_times:\n",
    "            return {\"conversations\": 0, \"avg_response_time\": 0}\n",
    "        \n",
    "        return {\n",
    "            \"conversations\": self.conversation_count,\n",
    "            \"avg_response_time\": sum(self.response_times) / len(self.response_times),\n",
    "            \"fastest_response\": min(self.response_times),\n",
    "            \"slowest_response\": max(self.response_times),\n",
    "            \"total_history\": len(self.conversation_history)\n",
    "        }\n",
    "    \n",
    "    def get_conversation_history(self, last_n: int = 5):\n",
    "        \"\"\"Get recent conversation history\"\"\"\n",
    "        return self.conversation_history[-last_n:] if self.conversation_history else []\n",
    "\n",
    "# Create the customer service agent\n",
    "agent = DataFlowCustomerAgent(rag_chain)\n",
    "print(\"âœ… Customer service agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Customer Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ­ TESTING CUSTOMER SERVICE SCENARIOS\n",
      "=============================================\n",
      "\n",
      "ğŸ“ Scenario 1: Billing\n",
      "â“ Question: What are your pricing plans and how much does the premium plan cost?\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Agent Response:\n",
      "   Our Premium support add-on is part of our Professional Plan, which costs $500/month. This plan includes priority escalation and a dedicated Customer Success Manager (CSM). If you're interested in learning more about our other pricing plans or would like to discuss customization options, I'd be happy to help!\n",
      "\n",
      "ğŸ“š Sources Used:\n",
      "   1. ğŸ“ business_data - billing_and_pricing.csv\n",
      "   2. ğŸ“ business_data - billing_and_pricing.csv\n",
      "   3. ğŸ“ business_data - billing_and_pricing.csv\n",
      "\n",
      "â±ï¸ Response Time: 2.54 seconds\n",
      "ğŸ¯ Department Accuracy: âœ… Accurate\n",
      "\n",
      "ğŸ“ Scenario 2: Technical Support\n",
      "â“ Question: How do I authenticate with your API? I'm getting authentication errors.\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Agent Response:\n",
      "   I'd be happy to help you authenticate with our API! There are a few common reasons that might cause authentication issues, so let's go through them together.\n",
      "\n",
      "Firstly, it's possible that your API key is invalid or has expired. To check this, please refer to the product user guide (Section 2.3) which provides instructions on how to generate and manage keys in Settings. If you're still having trouble, feel free to contact support for further assistance.\n",
      "\n",
      "If your issue persists, it's also possible that your OAuth 2.0 token or scope is incorrect. You can try refreshing the token via the /auth/token endpoint. Please check out our API documentation (OAuth 2.0) for more information on this process.\n",
      "\n",
      "Additionally, ensure that you're including the X-API-Key header in your requests correctly. The format should be X-API-Key: your_key. If none of these solutions work, please reach out to us through our support channels so we can help you troubleshoot further or provide additional guidance.\n",
      "\n",
      "Before I go any further, is there anything specific you'd like me to elaborate on?\n",
      "\n",
      "ğŸ“š Sources Used:\n",
      "   1. ğŸ“ customer_facing - troubleshooting_guide.txt\n",
      "   2. ğŸ“ customer_facing - api_documentation.json\n",
      "   3. ğŸ“ customer_facing - api_documentation.json\n",
      "\n",
      "â±ï¸ Response Time: 7.42 seconds\n",
      "ğŸ¯ Department Accuracy: âœ… Accurate\n",
      "\n",
      "ğŸ“ Scenario 3: Privacy/Legal\n",
      "â“ Question: What data do you collect and how do you protect my privacy?\n",
      "--------------------------------------------------\n",
      "ğŸ¤– Agent Response:\n",
      "   Thank you for reaching out to DataFlow Solutions! I'm happy to help answer your question about data collection and privacy protection.\n",
      "\n",
      "We collect various types of data from our users, including:\n",
      "\n",
      "* Account/usage data (~500GB/month)\n",
      "* Marketing emails (with a consent rate of 80%)\n",
      "* Support tickets (~1,000/month with data)\n",
      "\n",
      "To protect your privacy, we follow the guidelines outlined in our Data Classification and Handling policy. We categorize our data into four levels:\n",
      "\n",
      "* Public: Shared freely with approval for marketing materials\n",
      "* Internal: Accessible via Okta for employee data and internal processes\n",
      "* Confidential: Encrypted in transit and at rest for customer data and financials\n",
      "* Restricted: Limited to Security Team and DevOps for encryption keys, audit logs, and stored in AWS KMS\n",
      "\n",
      "We also prioritize compliance with global privacy laws, including GDPR, CCPA, and HIPAA. For example:\n",
      "\n",
      "* Customer data is anonymized for analytics (privacy_policy.txt)\n",
      "* We process customer data only when necessary and with explicit consent (compliance_certifications.csv)\n",
      "* Healthcare customers' health data are handled in accordance with HIPAA regulations (Customer 1039, customer_analytics.csv)\n",
      "\n",
      "To ensure the security of your personal data, we use various measures such as:\n",
      "\n",
      "* Encryption\n",
      "* Access controls via role-based permissions\n",
      "* Regular security audits and updates\n",
      "\n",
      "If you have any specific concerns or questions about our data collection and privacy practices, I encourage you to reach out to us directly at privacy@dataflow.com or +1-800-555-1234. We're here to help!\n",
      "\n",
      "ğŸ“š Sources Used:\n",
      "   1. ğŸ“ legal_compliance - security_policies.txt\n",
      "   2. ğŸ“ legal_compliance - privacy_policy.txt\n",
      "   3. ğŸ“ legal_compliance - privacy_policy.txt\n",
      "\n",
      "â±ï¸ Response Time: 9.37 seconds\n",
      "ğŸ¯ Department Accuracy: âœ… Accurate\n"
     ]
    }
   ],
   "source": [
    "def test_customer_scenarios(agent):\n",
    "    \"\"\"Test agent with realistic customer service scenarios\"\"\"\n",
    "    \n",
    "    print(\"ğŸ­ TESTING CUSTOMER SERVICE SCENARIOS\")\n",
    "    print(\"=\" * 45)\n",
    "    \n",
    "    # Realistic customer questions\n",
    "    scenarios = [\n",
    "        {\n",
    "            \"question\": \"What are your pricing plans and how much does the premium plan cost?\",\n",
    "            \"category\": \"Billing\",\n",
    "            \"expected_dept\": \"business_data\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"How do I authenticate with your API? I'm getting authentication errors.\",\n",
    "            \"category\": \"Technical Support\",\n",
    "            \"expected_dept\": \"customer_facing\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What data do you collect and how do you protect my privacy?\",\n",
    "            \"category\": \"Privacy/Legal\",\n",
    "            \"expected_dept\": \"legal_compliance\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, scenario in enumerate(scenarios, 1):\n",
    "        print(f\"\\nğŸ“ Scenario {i}: {scenario['category']}\")\n",
    "        print(f\"â“ Question: {scenario['question']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Get agent response\n",
    "        response = agent.ask(scenario[\"question\"])\n",
    "        \n",
    "        print(f\"ğŸ¤– Agent Response:\")\n",
    "        print(f\"   {response['answer']}\")  # Show complete response\n",
    "        \n",
    "        print(f\"\\nğŸ“š Sources Used:\")\n",
    "        for j, source in enumerate(response['sources'][:3], 1):  # Show top 3 sources\n",
    "            print(f\"   {j}. ğŸ“ {source['department']} - {source['file']}\")\n",
    "        \n",
    "        print(f\"\\nâ±ï¸ Response Time: {response['response_time']:.2f} seconds\")\n",
    "        \n",
    "        # Check if correct department was used\n",
    "        dept_match = any(source['department'] == scenario['expected_dept'] for source in response['sources'])\n",
    "        accuracy = \"âœ… Accurate\" if dept_match else \"âš ï¸ Needs Review\"\n",
    "        print(f\"ğŸ¯ Department Accuracy: {accuracy}\")\n",
    "        \n",
    "        results.append({\n",
    "            \"scenario\": scenario,\n",
    "            \"response\": response,\n",
    "            \"accurate\": dept_match\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the scenarios\n",
    "test_results = test_customer_scenarios(agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
